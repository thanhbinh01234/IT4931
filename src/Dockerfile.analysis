FROM continuumio/miniconda3 as python_env
WORKDIR /
COPY environment.yml .
RUN conda env create -f environment.yml

# FROM bde2020/spark-submit:3.0.1-hadoop3.2
FROM python_env
WORKDIR /code

RUN mkdir -p /usr/share/man/man1 && \
    apt-get update && \
    apt-get -y upgrade && \
    dpkg --configure -a && \
    apt-get install -y default-jdk scala wget && \
    apt-get clean

RUN wget -nv https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz && \
    tar xzf spark-3.0.1-bin-hadoop3.2.tgz && \
    mv spark-3.0.1-bin-hadoop3.2 /opt/spark

ENV CONDA_ENV myenv
ENTRYPOINT [ "/code/entry.sh" ]

CMD /opt/spark/bin/spark-submit src/analysis.py

# ENV SPARK_APPLICATION_PYTHON_LOCATION /code/src/analysis.py
# ENV SPARK_MASTER_NAME sparkmaster

# ONBUILD RUN cd /code \
#     && pip3 install -r requirements.txt

# CMD ["/bin/bash", "/submit.sh"]
